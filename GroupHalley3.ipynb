{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Items Needed\n",
    "\n",
    "#### For each agent, need\n",
    "- average reward\n",
    "- success rate (unsure how this differs from avg reward)\n",
    "- include the strategy used by each agent and accompanying hyper-parameters for training (learning rate, discount, etc)\n",
    "#### Design Breakdown\n",
    "- Text Descriptions (javadoc style)\n",
    "    - QLearningAgent (class)\n",
    "        - choose_action (func)\n",
    "        - update_q_table (func)\n",
    "    - Environment (class)\n",
    "        - reset (func)\n",
    "        - move_agent (func)\n",
    "    - train_agents (func)\n",
    "- State diagram to show relationships\n",
    "\n",
    "#### Each Experiment\n",
    "- experiment summary\n",
    "- explain results \n",
    "- show q-table for each agent\n",
    "- show stats for each agent (see item 1)\n",
    "\n",
    "#### Code comments (javadoc style)\n",
    "#### readme file\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT class\n",
    "\n",
    "The QLearningAgent class implements a Q-learning agent for reinforcement learning tasks. It includes methods for choosing actions based on different policies, updating the Q-table, and decaying the exploration rate over time.\n",
    "\n",
    "Member Variables:\n",
    "- **num_actions:** movement options for the agent (up, down, left, right)\n",
    "- **learning_rate (alpha):** determines to what extent new info overrides old info in q-val updates\n",
    "- **discount_factor (gamma):** determines importance of future rewards compared to immediate rewards\n",
    "- **exploration_rate (epsilon):** determines balance between exploration (trying new actions) and exploitaion (using known high q-val actions)\n",
    "- **exploration_decay:** gradually reduces exploration rate from exploration towards exploitation so that as the model continues it settles onto the best of what it knows\n",
    "\n",
    "chose_action (function):\n",
    "- redirects to prandom, pexploit, or pgreedy based on the policy set in the argument\n",
    "- **prandom:** randomly selects a valid action (used both as an execution policy and as a training/exploration policy)\n",
    "- **pexploit:** will either pick the best q-val action available (80% chance) or pick randomly (20% chance) - this gives the execution a balance between using optimized paths and potentially finding new paths\n",
    "- **pgreedy:** only picks the best known q-val options\n",
    "\n",
    "update_q_table (function):\n",
    "- updates the q-table with reward and predicted rewards for the next state (used when training the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Agent class\n",
    "class Agent:\n",
    "    def __init__(self, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.99):\n",
    "        # Initialize the Agent with default or provided parameters\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((25, num_actions))\n",
    "\n",
    "    # Method to choose action based on given policy\n",
    "    def choose_action(self, state, policy):\n",
    "        if policy == 'random':\n",
    "            return self.choose_action_random(state)\n",
    "        elif policy == 'exploit':\n",
    "            return self.choose_action_exploit(state)\n",
    "        elif policy == 'greedy':\n",
    "            return self.choose_action_greedy(state)\n",
    "\n",
    "    # Method to choose action randomly\n",
    "    def choose_action_random(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    # Method to choose action exploiting the Q-table\n",
    "    def choose_action_exploit(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            if np.random.rand() < 0.8:\n",
    "                return np.argmax(self.q_table[state])\n",
    "            else:\n",
    "                return np.random.choice(np.where(self.q_table[state] == np.max(self.q_table[state]))[0])\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    # Method to choose greedy action\n",
    "    def choose_action_greedy(self, state):\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    # Method to update Q-table based on the observed transition\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        old_q_value = self.q_table[state, action]\n",
    "        max_next_q_value = np.max(self.q_table[next_state])\n",
    "        new_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value - old_q_value)\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "    # Method to decay exploration rate\n",
    "    def decay_exploration_rate(self):\n",
    "        self.exploration_rate *= self.exploration_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Q Agent (class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SARSAQAgent class is designed for implementing the SARSA (State-Action-Reward-State-Action) algorithm within a Q-learning framework. SARSA is an on-policy learning algorithm that updates Q-values based on the current policy's action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SARSA Agent class, inheriting from Agent\n",
    "class SARSA_Agent(Agent):\n",
    "    # Override the update_q_table method for SARSA\n",
    "    def update_q_table(self, state, action, reward, next_state, next_action=None):\n",
    "        if next_action is None:\n",
    "            raise ValueError(\"Next action must be provided for SARSA\")\n",
    "        old_q_value = self.q_table[state, action]\n",
    "        next_q_value = self.q_table[next_state, next_action]\n",
    "        new_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * next_q_value - old_q_value)\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "    # Override the choose_action method for SARSA\n",
    "    def choose_action(self, state, policy):\n",
    "        if policy == 'random':\n",
    "            return self.choose_action_random(state)\n",
    "        elif policy == 'exploit':\n",
    "            return self.choose_action_exploit(state)\n",
    "        elif policy == 'greedy':\n",
    "            return self.choose_action_greedy(state)\n",
    "        elif policy == 'SARSA':\n",
    "            return self.choose_action_sarsa(state)\n",
    "\n",
    "    # Method to choose action using SARSA strategy\n",
    "    def choose_action_sarsa(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World class\n",
    "\n",
    "The Environment class represents the environment in which agents operate in a q-learning program. It defines the grid size, agent and block configurations, pickup and dropoff locations, and methods to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Environment class\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        # Initialize environment parameters\n",
    "        self.grid_size = 5\n",
    "        self.num_agents = 3\n",
    "        self.pickup_locations = [(1, 5), (2, 4), (5, 2)]  \n",
    "        self.dropoff_locations = [(1, 1), (3, 1), (4, 5)] \n",
    "        self.agent_locations = [(3, 3), (5, 3), (1, 3)]\n",
    "        self.agent_colors = ['red', 'blue', 'black']  \n",
    "        self.agent_blocks = [0, 0, 0]\n",
    "        self.blocks_at_pickup = [5, 5, 5]\n",
    "        self.max_blocks_at_dropoff = 5\n",
    "        self.paths = [[] for _ in range(self.num_agents)]\n",
    "        \n",
    "        # Validate pickup and drop-off locations\n",
    "        self.validate_locations(self.pickup_locations)\n",
    "        self.validate_locations(self.dropoff_locations)\n",
    "        \n",
    "    # Method to validate locations\n",
    "    def validate_locations(self, locations):\n",
    "        for loc in locations:\n",
    "            if not (1 <= loc[0] <= self.grid_size) or not (1 <= loc[1] <= self.grid_size):\n",
    "                raise ValueError(\"Location out of grid bounds\")\n",
    "\n",
    "    # Method to reset environment\n",
    "    def reset(self):\n",
    "        self.agent_locations = [(3, 3), (5, 3), (1, 3)]\n",
    "        self.agent_blocks = [0, 0, 0]\n",
    "        self.blocks_at_pickup = [5, 5, 5]\n",
    "        self.paths = [[] for _ in range(self.num_agents)]\n",
    "\n",
    "    # Method to get state for a given agent\n",
    "    def get_state(self, agent_id):\n",
    "        agent_loc = self.agent_locations[agent_id]\n",
    "        return (agent_loc[0] - 1) * (self.grid_size - 1) + (agent_loc[1] - 1)  \n",
    "\n",
    "    # Method to move agent and compute reward\n",
    "    def move_agent(self, agent_id, action):\n",
    "        # Move agent\n",
    "        current_loc = self.agent_locations[agent_id]\n",
    "        new_loc = current_loc\n",
    "        # Update location based on action\n",
    "        if action == 0:  \n",
    "            new_loc = (max(current_loc[0] - 1, 1), current_loc[1])\n",
    "        elif action == 1:  \n",
    "            new_loc = (min(current_loc[0] + 1, self.grid_size), current_loc[1])\n",
    "        elif action == 2:  \n",
    "            new_loc = (current_loc[0], max(current_loc[1] - 1, 1))\n",
    "        elif action == 3:  \n",
    "            new_loc = (current_loc[0], min(current_loc[1] + 1, self.grid_size))\n",
    "        # Prevent collisions\n",
    "        if new_loc in self.agent_locations:\n",
    "            new_loc = current_loc\n",
    "        # Update agent location\n",
    "        self.agent_locations[agent_id] = new_loc\n",
    "        self.paths[agent_id].append(new_loc)\n",
    "        # Compute reward\n",
    "        reward = -1\n",
    "        if new_loc in self.pickup_locations:\n",
    "            pickup_index = self.pickup_locations.index(new_loc)\n",
    "            if self.blocks_at_pickup[pickup_index] > 0 and self.agent_blocks[agent_id] == 0:\n",
    "                reward += 13\n",
    "                self.blocks_at_pickup[pickup_index] -= 1\n",
    "                self.agent_blocks[agent_id] += 1\n",
    "        elif new_loc in self.dropoff_locations:\n",
    "            dropoff_index = self.dropoff_locations.index(new_loc)\n",
    "            if self.agent_blocks[agent_id] > 0 and self.agent_blocks[agent_id] < self.max_blocks_at_dropoff:\n",
    "                reward += 13\n",
    "                self.agent_blocks[agent_id] -= 1\n",
    "        return reward\n",
    "    \n",
    "    # Method to compute Manhattan distance between two locations\n",
    "    def manhattan_distance(self, loc1, loc2):\n",
    "        return abs(loc1[0] - loc2[0]) + abs(loc1[1] - loc2[1])\n",
    "\n",
    "    # Method to get average Manhattan distance between agents and drop-off locations\n",
    "    def get_avg_manhattan_distance(self):\n",
    "        total_distance = 0\n",
    "        for agent_id in range(self.num_agents):\n",
    "            agent_loc = self.agent_locations[agent_id]\n",
    "            dropoff_loc = self.dropoff_locations[agent_id]\n",
    "            total_distance += self.manhattan_distance(agent_loc, dropoff_loc)\n",
    "        return total_distance / self.num_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution and Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_agents\n",
    "The train_agents function orchestrates the training process for multiple agents in a q-learning program. It iterates through training cycles, resets the environment, and updates agent actions and Q-tables based on specified policies.\n",
    "\n",
    "#### plot_q_table_heatmap\n",
    "The plot_q_table_heatmap function is used to visualize the Q-table as a heatmap, which helps in understanding the learned Q-values for different states and actions in the reinforcement learning environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train agents\n",
    "def train_agents(env, agents, num_iterations, policy):\n",
    "    for _ in range(2):  \n",
    "        for iteration in range(num_iterations):\n",
    "            env.reset()  \n",
    "            for agent_id in range(env.num_agents):\n",
    "                agent = agents[agent_id]\n",
    "                state = env.get_state(agent_id)\n",
    "                total_reward = 0\n",
    "                action = None  \n",
    "                for _ in range(100):  \n",
    "                    if policy == 'SARSA':\n",
    "                        next_action = agent.choose_action(state, policy)  \n",
    "                    action = agent.choose_action(state, policy)  \n",
    "                    reward = env.move_agent(agent_id, action)\n",
    "                    next_state = env.get_state(agent_id)\n",
    "                    total_reward += reward\n",
    "                    if policy == 'SARSA':\n",
    "                        agent.update_q_table(state, action, reward, next_state, next_action)  \n",
    "                    else:\n",
    "                        agent.update_q_table(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "                    if total_reward > 0:  \n",
    "                        break\n",
    "                agent.decay_exploration_rate()\n",
    "\n",
    "# Define a function to plot Q-table heatmap\n",
    "def plot_q_table_heatmap(q_table, action_labels, vmin=None, vmax=None, state_labels=None, action_ticks=False):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    im = plt.imshow(q_table, cmap='viridis',\n",
    "                    aspect='auto', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label='Q-Value')\n",
    "    plt.xticks(np.arange(len(action_labels)), action_labels,\n",
    "               rotation=45) if action_labels else plt.xticks([])\n",
    "    plt.xlabel('Actions')\n",
    "    if state_labels:\n",
    "        plt.yticks(np.arange(len(state_labels)), state_labels)\n",
    "    plt.ylabel('States')\n",
    "    if action_ticks and action_labels:\n",
    "        plt.xticks(np.arange(len(action_labels)), action_labels)\n",
    "    num_states, num_actions = q_table.shape\n",
    "    for i in range(num_states + 1):\n",
    "        plt.axhline(i - 0.5, color='black', linewidth=0.5)\n",
    "    for j in range(num_actions + 1):\n",
    "        plt.axvline(j - 0.5, color='black', linewidth=0.5)\n",
    "    plt.title('Q-Table Heatmap')\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment 1\n",
    "def experiment_1(env, agents):\n",
    "    action_labels = ['North', 'South', 'West', 'East']\n",
    "    print(\"Experiment 1:\")\n",
    "    train_agents(env, agents, num_iterations=500, policy='random')  \n",
    "    train_agents(env, agents, num_iterations=8500, policy='random')\n",
    "    train_agents(env, agents, num_iterations=8500, policy='greedy')\n",
    "    train_agents(env, agents, num_iterations=8500, policy='exploit')\n",
    "\n",
    "    print(\"Q-table for exploit policy:\")\n",
    "    for agent_id, agent in enumerate(agents):\n",
    "        agent_color = env.agent_colors[agent_id]  \n",
    "        print(f\"Agent {agent_id} (Color: {agent_color}) Q-table:\")\n",
    "        print(agent.q_table)\n",
    "        plot_q_table_heatmap(agent.q_table, action_labels)\n",
    "        \n",
    "    avg_distance = env.get_avg_manhattan_distance()\n",
    "    print(\"Average Manhattan Distance:\", avg_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define experiment 2\n",
    "def experiment_2(env, q_learning_agents, sarsa_agents):\n",
    "    action_labels = ['North', 'South', 'West', 'East']\n",
    "    print(\"Experiment 2 with Q-learning:\")\n",
    "    train_agents(env, q_learning_agents, num_iterations=9000, policy='greedy')\n",
    "\n",
    "    print(\"Experiment 2 with SARSA:\")\n",
    "    train_agents(env, sarsa_agents, num_iterations=9000, policy='SARSA')\n",
    "\n",
    "    print(\"Q-table for SARSA policy:\")\n",
    "    for agent_id, agent in enumerate(sarsa_agents):\n",
    "        agent_color = env.agent_colors[agent_id]  \n",
    "        print(f\"Agent {agent_id} (Color: {agent_color}) Q-table:\")\n",
    "        print(agent.q_table)\n",
    "        plot_q_table_heatmap(agent.q_table, action_labels)\n",
    "        \n",
    "    avg_distance = env.get_avg_manhattan_distance()\n",
    "    print(\"Average Manhattan Distance:\", avg_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment 3\n",
    "def experiment_3(env, learning_rates):\n",
    "    action_labels = ['North', 'South', 'West', 'East']\n",
    "    for i, alpha in enumerate(learning_rates):\n",
    "        print(f\"Experiment 3 with Q-learning (Learning Rate: {alpha}):\")\n",
    "        q_learning_agents = [Agent(num_actions=4, learning_rate=alpha, discount_factor=0.5) for _ in range(env.num_agents)]\n",
    "        train_agents(env, q_learning_agents, num_iterations=9000, policy='exploit')\n",
    "        for agent_id, agent in enumerate(q_learning_agents):\n",
    "            agent_color = env.agent_colors[agent_id]\n",
    "            print(f\"Agent {agent_id} (Color: {agent_color}) Q-table:\")\n",
    "            print(agent.q_table)\n",
    "            plot_q_table_heatmap(agent.q_table, action_labels)\n",
    "            \n",
    "    avg_distance = env.get_avg_manhattan_distance()\n",
    "    print(\"Average Manhattan Distance:\", avg_distance)\n",
    "\n",
    "    for i, alpha in enumerate(learning_rates):\n",
    "        print(f\"Experiment 3 with SARSA (Learning Rate: {alpha}):\")\n",
    "        sarsa_agents = [SARSA_Agent(num_actions=4, learning_rate=alpha, discount_factor=0.5) for _ in range(env.num_agents)]\n",
    "        train_agents(env, sarsa_agents, num_iterations=9000, policy='SARSA')\n",
    "        for agent_id, agent in enumerate(sarsa_agents):\n",
    "            agent_color = env.agent_colors[agent_id]\n",
    "            print(f\"Agent {agent_id} (Color: {agent_color}) Q-table:\")\n",
    "            print(agent.q_table)\n",
    "            plot_q_table_heatmap(agent.q_table, action_labels)\n",
    "        \n",
    "    avg_distance = env.get_avg_manhattan_distance()\n",
    "    print(\"Average Manhattan Distance:\", avg_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment 4\n",
    "def experiment_4(env, agents):\n",
    "    action_labels = ['North', 'South', 'West', 'East']\n",
    "    print(\"Experiment 4:\")\n",
    "    train_agents_exp4(env, agents, num_iterations=10000, policy='random')  \n",
    "    train_agents_exp4(env, agents, num_iterations=15000, policy='exploit')  \n",
    "\n",
    "    print(\"Q-tables after Experiment 4:\")\n",
    "    for agent_id, agent in enumerate(agents):\n",
    "        agent_color = env.agent_colors[agent_id]  \n",
    "        print(f\"Agent {agent_id} (Color: {agent_color}) Q-table:\")\n",
    "        print(agent.q_table)\n",
    "        plot_q_table_heatmap(agent.q_table, action_labels)\n",
    "        \n",
    "    avg_distance = env.get_avg_manhattan_distance()\n",
    "    print(\"Average Manhattan Distance:\", avg_distance)\n",
    "\n",
    "# Define training method for experiment 4\n",
    "def train_agents_exp4(env, agents, num_iterations, policy):\n",
    "    terminal_state_count = 0  \n",
    "    for iteration in range(num_iterations):\n",
    "        env.reset()  \n",
    "        for agent_id in range(env.num_agents):\n",
    "            agent = agents[agent_id]\n",
    "            state = env.get_state(agent_id)\n",
    "            total_reward = 0\n",
    "            action = None  \n",
    "            for _ in range(100):  \n",
    "                if policy == 'SARSA':\n",
    "                    next_action = agent.choose_action(state, policy)  \n",
    "                action = agent.choose_action(state, policy)  \n",
    "                reward = env.move_agent(agent_id, action)\n",
    "                next_state = env.get_state(agent_id)\n",
    "                total_reward += reward\n",
    "                if policy == 'SARSA':\n",
    "                    agent.update_q_table(state, action, reward, next_state, next_action)  \n",
    "                else:\n",
    "                    agent.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                if total_reward > 0:  \n",
    "                    break\n",
    "            agent.decay_exploration_rate()\n",
    "            if total_reward > 0:\n",
    "                terminal_state_count += 1\n",
    "                if terminal_state_count == 3:\n",
    "                    env.pickup_locations = [(4, 2), (3, 3), (2, 4)]\n",
    "            if terminal_state_count == 6:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function to execute experiments\n",
    "def main():\n",
    "    env = Environment()\n",
    "    agents = [Agent(num_actions=4, learning_rate=0.3, discount_factor=0.5) for _ in range(env.num_agents)]\n",
    "    q_learning_agents = [Agent(num_actions=4, learning_rate=0.3, discount_factor=0.5) for _ in range(env.num_agents)]\n",
    "    sarsa_agents = [SARSA_Agent(num_actions=4, learning_rate=0.3, discount_factor=0.5) for _ in range(env.num_agents)]\n",
    "    learning_rates = [0.15, 0.45]\n",
    "\n",
    "    experiment_1(env, agents)\n",
    "    # experiment_2(env, q_learning_agents, sarsa_agents)\n",
    "    # experiment_3(env, learning_rates)\n",
    "    # experiment_4(env, agents)\n",
    "\n",
    "# Execute main function if this script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
